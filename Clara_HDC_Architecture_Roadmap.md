# Clara HDC Architecture Roadmap

## Executive Summary

This document outlines proposed architectural upgrades for Clara, an AI agent with a dual-brain architecture (Mistral personality + Phi-3 knowledge) using Hyperdimensional Computing (HDC) for memory. These recommendations emerged from development sessions focused on improving memory recall, personality consistency, and scalability.

---

## Current Architecture

```
                              USER QUERY
                                  â”‚
                                  â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    SEMANTIC ROUTER      â”‚
                    â”‚  (all-MiniLM-L6-v2)     â”‚
                    â”‚                         â”‚
                    â”‚  Embeds query, compares â”‚
                    â”‚  to domain descriptions â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                                         â”‚
            â–¼                                         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ PERSONALITY     â”‚                     â”‚ KNOWLEDGE       â”‚
   â”‚ BRAIN           â”‚                     â”‚ BRAIN           â”‚
   â”‚                 â”‚                     â”‚                 â”‚
   â”‚ Mistral 7B      â”‚                     â”‚ Phi-3 (merged)  â”‚
   â”‚ + LoRA adapters â”‚                     â”‚                 â”‚
   â”‚   â€¢ warmth      â”‚                     â”‚ Domains:        â”‚
   â”‚   â€¢ playful     â”‚                     â”‚   â€¢ medical     â”‚
   â”‚   â€¢ encouragementâ”‚                    â”‚   â€¢ coding      â”‚
   â”‚                 â”‚                     â”‚   â€¢ teaching    â”‚
   â”‚                 â”‚                     â”‚   â€¢ quantum     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**HDC Memory Layer (v2.1):**
- 10,000-dimension bipolar hypervectors
- Semantic personality vectors (encoded from trait descriptions)
- Entity extraction and indexing
- Personality-based importance boosting
- Memory context injection into prompts

---

## Proposed Upgrades

### 1. 64k-Dimension Vectors + Bundle Merging

**Current:** 10,000-dimension vectors  
**Proposed:** 64,000-dimension vectors (configurable)

| Dimension | Memory/vector | Noise Tolerance | Use Case |
|-----------|---------------|-----------------|----------|
| 10k-dim   | 40 KB         | Good            | Edge (Jetson), <1000 memories |
| 64k-dim   | 256 KB        | Excellent       | Server, long-term, 10k+ memories |

**Benefits:**
- HDC capacity scales as O(d / log d) â€” 64k provides ~6x more binding capacity
- Finer clustering for distinguishing similar but distinct memories
- Better regime detection (recognizing conversation states)
- Improved noise resistance

**Bundle Merging (HDC's Killer Feature):**
```python
# Vector DB: Must rebuild index or use approximate methods
# HDC: 1-shot update, O(d) operation
memory_bundle = bundle([memory_bundle, new_memory_hv])  # Done!
```

**Implementation:**
```python
class ClaraHDCMemory:
    def __init__(self, embedder, dim: int = 10000, ...):
        # Easy to swap: dim=64000 for production
```

**Effort:** Low | **Impact:** Medium | **Priority:** âœ… Easy win

---

### 2. Alternative Router: Nemotron-Nano-2B

**Current:** Semantic router using all-MiniLM-L6-v2 embeddings  
**Alternative:** NVIDIA Nemotron-Nano-2B (open-source, Hugging Face)

| Router Type | Latency | Accuracy | Interpretability |
|-------------|---------|----------|------------------|
| Semantic (current) | ~5ms | 93%+ | High |
| Nemotron-Nano-2B | ~50-100ms | Higher? | Medium |
| HDC Router | ~2ms | TBD | Very high |

**Where Nemotron Shines:**
- Complex/ambiguous queries
- Multi-intent detection
- Context-dependent routing

**Recommended Hybrid Approach:**
```
Query â†’ Fast semantic router â†’ Confidence check
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                               â”‚
              Conf > 0.4                      Conf < 0.4
                    â”‚                               â”‚
                    â–¼                               â–¼
            Direct routing              Nemotron deliberation
              (5ms)                          (50ms)
```

**Alternative Architecture (Simpler):**
Use Nemotron as unified base with domain LoRA adapters:
```
Nemotron-Nano-2B (base)
â”œâ”€â”€ personality_lora
â”œâ”€â”€ coding_lora
â”œâ”€â”€ medical_lora
â”œâ”€â”€ teaching_lora
â””â”€â”€ quantum_lora
```

**Effort:** High | **Impact:** Medium | **Priority:** â³ Evaluate vs. current

---

### 3. Voice Fine-Tuning from Chat History

**Goal:** Create distinctive "Clara voice" from conversation history  
**Data:** ~100k tokens of chat history  
**Method:** LoRA fine-tuning on base model

| Token Count | Quality |
|-------------|---------|
| 10k         | Basic patterns, inconsistent |
| 50k         | Recognizable voice, some gaps |
| 100k        | Strong voice adapter âœ“ |
| 500k+       | Very consistent, risk of overfitting |

**Key Considerations:**
- Data quality > quantity
- Diversity of topics, emotions, response lengths
- Lower LoRA rank for style vs. knowledge (r=16 or r=32)

**Recommended Configuration:**
```python
voice_lora_config = LoraConfig(
    r=16,                    # Lower rank for style
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # Attention only
    lora_dropout=0.05,
)

# Training format:
# Human: [message]
# Clara: [response in Clara's voice]
```

**100k tokens â‰ˆ 2,000-3,000 conversation turns** â€” sufficient if representative.

**Effort:** Medium | **Impact:** High | **Priority:** âœ… Differentiator

---

### 4. Recursive Reflection

**Concept:** Generate response, then self-edit for tone/consistency  
**Implementation:** Conditional "System 2" thinking for complex queries

**Pipeline:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  GENERATION PIPELINE                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  Query â†’ [Simple queries: fast path]               â”‚
â”‚              â”‚                                      â”‚
â”‚              â–¼                                      â”‚
â”‚         Generate response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Output
â”‚              â”‚                                      â”‚
â”‚         [Complex/important: slow path]             â”‚
â”‚              â”‚                                      â”‚
â”‚              â–¼                                      â”‚
â”‚         Reflect & refine                            â”‚
â”‚         "Review for warmth, accuracy, tone"        â”‚
â”‚              â”‚                                      â”‚
â”‚              â–¼                                      â”‚
â”‚         Revised response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Output
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Reflection Triggers:**
- Response > 200 tokens
- Mixed emotion + technical query
- Low routing confidence
- User explicitly requests detail

**Implementation Sketch:**
```python
def clara_with_reflection(query: str, reflect: bool = "auto") -> str:
    response = clara(query, store_interaction=False)
    
    if reflect == "auto":
        reflect = should_reflect(query, response)
    
    if reflect:
        reflection_prompt = f"""Review this response for Clara's voice:
        
Original: {response}

Check for:
1. Warmth and encouragement
2. Patience
3. Clarity

Provide improved response:"""
        
        response = generate(reflection_prompt)
    
    store_interaction(query, response)
    return response
```

**Tradeoff:** ~2x latency for reflected responses

**Effort:** Medium | **Impact:** Medium | **Priority:** â³ After basics work

---

### 5. Chain-of-Thought Prompting

**Concept:** Structured thinking before response generation  
**Implementation:** Prompt engineering rather than architectural change

**CoT Prompt Template:**
```python
COT_PROMPT = """Think through this step by step:
1. What is the user really asking?
2. What do I know from memory that's relevant?
3. What domain expertise applies?
4. How would Clara (warm, patient, encouraging) phrase this?

User query: {query}

My thinking:
"""
```

**Complexity-Gated Generation:**
```python
def smart_generate(query, complexity_score):
    if complexity_score < 0.3:
        return direct_generate(query)        # Fast path
    elif complexity_score < 0.7:
        return cot_generate(query)           # Think first
    else:
        return cot_reflect_generate(query)   # Full pipeline
```

**Effort:** Low | **Impact:** Low | **Priority:** â³ Prompt engineering

---

### 6. Memory Tiers with HDC

**Concept:** Session / Daily / Long-term memory with elegant HDC-based retrieval  
**Key Insight:** HDC naturally blends tiers via similarity weighting â€” no rigid boundaries

**Tier Structure:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HDC MEMORY TIERS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  SESSION (Working Memory)                                   â”‚
â”‚  â”œâ”€ Last N turns (N=10)                                    â”‚
â”‚  â”œâ”€ Highest recall weight (1.0)                            â”‚
â”‚  â”œâ”€ No persistence                                         â”‚
â”‚  â””â”€ Bound with: SESSION âŠ— TURN_N âŠ— content                â”‚
â”‚                                                             â”‚
â”‚  DAILY (Episodic Buffer)                                   â”‚
â”‚  â”œâ”€ Today's consolidated memories                          â”‚
â”‚  â”œâ”€ Medium recall weight (0.7)                             â”‚
â”‚  â”œâ”€ Persists until "sleep" cycle                          â”‚
â”‚  â””â”€ Bound with: TODAY âŠ— TOPIC âŠ— content                   â”‚
â”‚                                                             â”‚
â”‚  LONG-TERM (Semantic Memory)                               â”‚
â”‚  â”œâ”€ High-importance consolidated facts                     â”‚
â”‚  â”œâ”€ Lower recall weight (0.5)                              â”‚
â”‚  â”œâ”€ Survives consolidation cycles                         â”‚
â”‚  â””â”€ Bound with: PERMANENT âŠ— DOMAIN âŠ— content              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**HDC Retrieval (No Rigid Boundaries):**
```python
def recall(self, query, tier_weights=None):
    tier_weights = tier_weights or {
        'session': 1.0,
        'daily': 0.7,
        'longterm': 0.5,
    }
    
    query_hv = self._text_to_hv(query)
    
    results = []
    for hv, memory in self.memories:
        base_sim = self.similarity(query_hv, hv)
        tier_weight = tier_weights.get(memory.tier, 0.5)
        
        # Blend: relevance Ã— tier Ã— importance Ã— recency
        score = base_sim * tier_weight * memory.importance * recency(memory)
        results.append((memory, score))
    
    return sorted(results, reverse=True)[:top_k]
```

**Consolidation ("Sleep" Cycle):**
```python
def consolidate(self):
    """Nightly consolidation cycle"""
    
    # 1. Session â†’ Daily
    for mem in self.session_memories:
        mem.tier = 'daily'
    
    # 2. Daily â†’ Long-term (high importance only)
    for mem in self.daily_memories:
        if mem.importance > 0.7:
            mem.tier = 'longterm'
            mem.text = extract_key_facts(mem.text)  # Compress
        elif mem.age_days > 7:
            self.forget(mem)  # Decay
    
    # 3. Pattern extraction
    patterns = extract_patterns(self.daily_memories)
    for pattern in patterns:
        self.store(pattern, tier='longterm', memory_type='pattern')
```

**Effort:** Medium | **Impact:** High | **Priority:** âœ… Core feature

---

## Implementation Priority

| Upgrade | Effort | Impact | Priority | Notes |
|---------|--------|--------|----------|-------|
| 64k-dim (configurable) | Low | Medium | âœ… P1 | Easy win, backward compatible |
| Memory tiers + consolidation | Medium | High | âœ… P1 | Core feature for long-term use |
| Voice LoRA (100k tokens) | Medium | High | âœ… P1 | Key differentiator |
| Recursive reflection | Medium | Medium | â³ P2 | After basics work |
| Nemotron router evaluation | High | Medium | â³ P2 | Compare against current |
| CoT loops | Low | Low | â³ P3 | Prompt engineering task |

---

## Current Implementation Status

### Completed (v2.1)
- âœ… Semantic router with 93%+ accuracy
- âœ… Dual-brain architecture (Mistral personality + Phi-3 knowledge)
- âœ… HDC memory with 10k-dim vectors
- âœ… Entity extraction and indexing
- âœ… Semantic personality vectors (encoded from trait descriptions)
- âœ… Personality-based importance boosting
- âœ… Memory context injection
- âœ… Memory persistence (save/load)
- âœ… Fixed recall threshold (0.15) for conversational follow-ups

### In Progress
- ğŸ”„ Memory tier implementation
- ğŸ”„ Consolidation cycle design

### Planned
- â³ 64k-dim vector support
- â³ Voice LoRA training pipeline
- â³ Recursive reflection (conditional)
- â³ Nemotron router evaluation

---

## Appendix: Clara Personality Specification

```python
CLARA_PERSONALITY_WEIGHTS = {
    'warmth': 0.85,       # Very warm, friendly
    'patience': 0.90,     # Very patient
    'curiosity': 0.75,    # Intellectually curious
    'encouragement': 0.85, # Very supportive
}
```

Each trait is encoded from rich semantic descriptions, enabling:
- Personality-aligned memory storage (warm interactions remembered more)
- Response alignment checking
- Consistent behavioral patterns

---

## Document History

| Version | Date | Changes |
|---------|------|---------|
| 1.0 | Dec 2024 | Initial architecture roadmap |

---

*This document reflects architectural decisions for the Clara AI agent project, part of the D.Eng research on HDC-based memory systems for edge AI deployment.*
