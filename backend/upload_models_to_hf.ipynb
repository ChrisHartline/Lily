{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Upload Clara Models to Hugging Face Hub\n",
        "\n",
        "This notebook uploads all Clara models from Google Drive to Hugging Face Hub.\n",
        "\n",
        "**Models:**\n",
        "- `clara-knowledge` - Phi-3 fine-tuned knowledge brain (~7GB)\n",
        "- `clara-warmth` - Mistral LoRA adapter (~52MB)\n",
        "- `clara-playful` - Mistral LoRA adapter (~52MB)\n",
        "- `clara-encouragement` - Mistral LoRA adapter (~52MB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Install dependencies\n",
        "!pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Login to Hugging Face\n",
        "# Get your token from: https://huggingface.co/settings/tokens\n",
        "# Make sure it has WRITE access!\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Configuration\n",
        "# UPDATE THESE PATHS to match your Google Drive structure!\n",
        "\n",
        "import os\n",
        "\n",
        "# Your Hugging Face username\n",
        "HF_USERNAME = \"ChrisHartline\"  # Change if different\n",
        "\n",
        "# Base path in Google Drive\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/Lily/models\"\n",
        "\n",
        "# Model configurations: (drive_folder, hf_repo_name, description)\n",
        "MODELS = [\n",
        "    {\n",
        "        \"drive_path\": f\"{DRIVE_BASE}/clara-knowledge\",\n",
        "        \"hf_repo\": f\"{HF_USERNAME}/clara-knowledge\",\n",
        "        \"description\": \"Clara Knowledge Brain - Phi-3 fine-tuned for medical, coding, teaching, quantum domains\",\n",
        "        \"tags\": [\"phi-3\", \"clara\", \"fine-tuned\", \"knowledge\"]\n",
        "    },\n",
        "    {\n",
        "        \"drive_path\": f\"{DRIVE_BASE}/mistral_warmth\",\n",
        "        \"hf_repo\": f\"{HF_USERNAME}/clara-warmth\",\n",
        "        \"description\": \"Clara Warmth Adapter - LoRA for warm, friendly personality\",\n",
        "        \"tags\": [\"mistral\", \"lora\", \"clara\", \"personality\", \"warmth\"]\n",
        "    },\n",
        "    {\n",
        "        \"drive_path\": f\"{DRIVE_BASE}/mistral_playful\",\n",
        "        \"hf_repo\": f\"{HF_USERNAME}/clara-playful\",\n",
        "        \"description\": \"Clara Playful Adapter - LoRA for playful, witty personality\",\n",
        "        \"tags\": [\"mistral\", \"lora\", \"clara\", \"personality\", \"playful\"]\n",
        "    },\n",
        "    {\n",
        "        \"drive_path\": f\"{DRIVE_BASE}/mistral_encouragement\",\n",
        "        \"hf_repo\": f\"{HF_USERNAME}/clara-encouragement\",\n",
        "        \"description\": \"Clara Encouragement Adapter - LoRA for supportive, motivating personality\",\n",
        "        \"tags\": [\"mistral\", \"lora\", \"clara\", \"personality\", \"encouragement\"]\n",
        "    },\n",
        "]\n",
        "\n",
        "# Verify paths exist\n",
        "print(\"Checking paths...\")\n",
        "for model in MODELS:\n",
        "    path = model[\"drive_path\"]\n",
        "    exists = os.path.exists(path)\n",
        "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
        "    print(f\"  {status} {path}\")\n",
        "    if exists:\n",
        "        files = os.listdir(path)\n",
        "        print(f\"      Files: {len(files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Upload function\n",
        "from huggingface_hub import HfApi, create_repo\n",
        "import shutil\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "def upload_model(drive_path, hf_repo, description, tags):\n",
        "    \"\"\"\n",
        "    Upload a model folder to Hugging Face Hub.\n",
        "    Skips checkpoint folders (training artifacts).\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Uploading: {hf_repo}\")\n",
        "    print(f\"From: {drive_path}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    if not os.path.exists(drive_path):\n",
        "        print(f\"‚ùå Path not found: {drive_path}\")\n",
        "        return False\n",
        "    \n",
        "    # Create repo if it doesn't exist\n",
        "    try:\n",
        "        create_repo(hf_repo, repo_type=\"model\", exist_ok=True)\n",
        "        print(f\"‚úÖ Repo created/exists: {hf_repo}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Repo creation: {e}\")\n",
        "    \n",
        "    # Create a clean copy without checkpoints\n",
        "    temp_path = f\"/content/temp_upload\"\n",
        "    if os.path.exists(temp_path):\n",
        "        shutil.rmtree(temp_path)\n",
        "    os.makedirs(temp_path)\n",
        "    \n",
        "    # Copy files (skip checkpoint folders)\n",
        "    for item in os.listdir(drive_path):\n",
        "        src = os.path.join(drive_path, item)\n",
        "        dst = os.path.join(temp_path, item)\n",
        "        \n",
        "        # Skip checkpoint folders\n",
        "        if item.startswith(\"checkpoint-\"):\n",
        "            print(f\"  ‚è≠Ô∏è Skipping: {item}\")\n",
        "            continue\n",
        "        \n",
        "        if os.path.isfile(src):\n",
        "            shutil.copy2(src, dst)\n",
        "            size_mb = os.path.getsize(src) / (1024*1024)\n",
        "            print(f\"  üìÑ {item} ({size_mb:.1f} MB)\")\n",
        "        elif os.path.isdir(src):\n",
        "            shutil.copytree(src, dst)\n",
        "            print(f\"  üìÅ {item}/\")\n",
        "    \n",
        "    # Create model card\n",
        "    model_card = f\"\"\"---\n",
        "tags:\n",
        "{chr(10).join(f'- {tag}' for tag in tags)}\n",
        "license: mit\n",
        "---\n",
        "\n",
        "# {hf_repo.split('/')[-1]}\n",
        "\n",
        "{description}\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"{hf_repo}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{hf_repo}\")\n",
        "```\n",
        "\n",
        "## Part of Clara\n",
        "\n",
        "This model is part of the Clara embodied AI system.\n",
        "See: https://github.com/ChrisHartline/embodiedMoE\n",
        "\"\"\"\n",
        "    \n",
        "    with open(os.path.join(temp_path, \"README.md\"), \"w\") as f:\n",
        "        f.write(model_card)\n",
        "    \n",
        "    # Upload\n",
        "    print(f\"\\nüì§ Uploading to {hf_repo}...\")\n",
        "    try:\n",
        "        api.upload_folder(\n",
        "            folder_path=temp_path,\n",
        "            repo_id=hf_repo,\n",
        "            repo_type=\"model\",\n",
        "            commit_message=f\"Upload {hf_repo.split('/')[-1]}\"\n",
        "        )\n",
        "        print(f\"‚úÖ Success! https://huggingface.co/{hf_repo}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Upload failed: {e}\")\n",
        "        return False\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        if os.path.exists(temp_path):\n",
        "            shutil.rmtree(temp_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Upload LoRA adapters first (smaller, faster)\n",
        "# These are the personality adapters (~52MB each)\n",
        "\n",
        "lora_models = [m for m in MODELS if \"lora\" in m.get(\"tags\", [])]\n",
        "\n",
        "print(f\"Uploading {len(lora_models)} LoRA adapters...\\n\")\n",
        "\n",
        "for model in lora_models:\n",
        "    upload_model(\n",
        "        model[\"drive_path\"],\n",
        "        model[\"hf_repo\"],\n",
        "        model[\"description\"],\n",
        "        model[\"tags\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Upload Knowledge Brain (large ~7GB, may take a while)\n",
        "# Run this cell separately - it takes longer\n",
        "\n",
        "knowledge_model = [m for m in MODELS if \"knowledge\" in m.get(\"tags\", [])][0]\n",
        "\n",
        "print(\"Uploading Knowledge Brain (~7GB - this may take 10-20 minutes)...\")\n",
        "print(\"‚òï Go grab a coffee!\\n\")\n",
        "\n",
        "upload_model(\n",
        "    knowledge_model[\"drive_path\"],\n",
        "    knowledge_model[\"hf_repo\"],\n",
        "    knowledge_model[\"description\"],\n",
        "    knowledge_model[\"tags\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Verify uploads\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"UPLOAD SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for model in MODELS:\n",
        "    repo = model[\"hf_repo\"]\n",
        "    try:\n",
        "        info = api.model_info(repo)\n",
        "        print(f\"\\n‚úÖ {repo}\")\n",
        "        print(f\"   URL: https://huggingface.co/{repo}\")\n",
        "        print(f\"   Files: {len(info.siblings)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå {repo} - Not found or error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Done! Update your backend to load from HuggingFace.\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
